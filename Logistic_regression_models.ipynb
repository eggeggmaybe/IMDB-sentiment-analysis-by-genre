{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "0837d0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a77181",
   "metadata": {},
   "source": [
    "# machine learning using all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "c0e3b83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = dict()\n",
    "\n",
    "with open('C:/Users/Administrator/Desktop/MBS/Module3/Text and Web Analytics/assignment/our_dataset/aclImdb/imdb.vocab', 'r') as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        # remove '\\n' at the end of each line\n",
    "        word = re.sub(r\"\\n\", '', line)\n",
    "        idx2word[idx] = word\n",
    "        \n",
    "word_rating = dict()\n",
    "\n",
    "# change lexicon for the line below if needed\n",
    "with open('C:/Users/Administrator/Desktop/MBS/Module3/Text and Web Analytics/assignment/My work/sort_reviews_by_genres/genres/train/lexicon/Horror.txt', 'r') as f:\n",
    "#with open('C:/Users/Administrator/Desktop/MBS/Module3/Text and Web Analytics/assignment/our_dataset/aclImdb/imdbEr.txt', 'r') as f:    \n",
    "    for idx, line in enumerate(f):\n",
    "        # remove '\\n' at the end of each line\n",
    "        rating = re.sub(r\"\\n\", '', line)\n",
    "        word_rating[idx2word[idx]] = float(rating)\n",
    "\n",
    "# word_rating = {word:rating, word:rating, .........}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0e5593",
   "metadata": {},
   "source": [
    "# BOW models (General model 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f8bda401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data in BOW\n",
    "f = open('C:/Users/Administrator/Desktop/MBS/Module3/Text and Web Analytics/assignment/our_dataset/aclImdb/train/labeledBow.feat','r')\n",
    "X_train_bow = []\n",
    "y_train_bow = []\n",
    "\n",
    "for line in f:\n",
    "    fields=re.split(r'\\s+',line)\n",
    "    # Positive reviews have 7 or more stars, negative reviews have <4\n",
    "    if (int(fields[0])>=7):\n",
    "        y_train_bow.append(\"1\")\n",
    "    else:\n",
    "        y_train_bow.append(\"0\")\n",
    "\n",
    "    # this_BOW will store all of the term frequencies for this line in a dictionary\n",
    "    # let's start by attempting to process all of the TDM entries\n",
    "    this_BOW = {}\n",
    "    for i in range(1, len(fields)-1):\n",
    "        index, val = re.split(r':',fields[i])\n",
    "        word = idx2word[int(index)]\n",
    "        this_BOW[word] = int(val)\n",
    "        \n",
    "    X_train_bow.append(this_BOW)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f58e9f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "f = open('C:/Users/Administrator/Desktop/MBS/Module3/Text and Web Analytics/assignment/our_dataset/aclImdb/test/labeledBow.feat','r')\n",
    "X_test_bow = []\n",
    "y_test_bow = []\n",
    "\n",
    "for line in f:\n",
    "    fields=re.split(r'\\s+',line)\n",
    "    # Positive reviews have 7 or more stars, negative reviews have <4\n",
    "    if (int(fields[0])>=7):\n",
    "            y_test_bow.append(\"1\")\n",
    "    else:\n",
    "            y_test_bow.append(\"0\")\n",
    "\n",
    "    # this_BOW will store all of the term frequencies for this line in a dictionary\n",
    "    # let's start by attempting to process all of the TDM entries\n",
    "    this_BOW = {}\n",
    "    for i in range(1, len(fields)-1):\n",
    "        index, val = re.split(r':',fields[i])\n",
    "        word = idx2word[int(index)]\n",
    "        this_BOW[word] = int(val)\n",
    "        \n",
    "    X_test_bow.append(this_BOW)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "85030981",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = DictVectorizer()\n",
    "# Only use fit_transform() for the training set to ensure that feature set is determined by the training set\n",
    "X_train_bow = vectorizer.fit_transform(X_train_bow)\n",
    "X_test_bow = vectorizer.transform(X_test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "87441819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6254\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.54      0.59     12500\n",
      "           1       0.61      0.71      0.66     12500\n",
      "\n",
      "    accuracy                           0.63     25000\n",
      "   macro avg       0.63      0.63      0.62     25000\n",
      "weighted avg       0.63      0.63      0.62     25000\n",
      "\n",
      "Accuracy:  0.84172\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.84      0.84     12500\n",
      "           1       0.84      0.84      0.84     12500\n",
      "\n",
      "    accuracy                           0.84     25000\n",
      "   macro avg       0.84      0.84      0.84     25000\n",
      "weighted avg       0.84      0.84      0.84     25000\n",
      "\n",
      "Accuracy:  0.8136\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.88      0.82     12500\n",
      "           1       0.86      0.75      0.80     12500\n",
      "\n",
      "    accuracy                           0.81     25000\n",
      "   macro avg       0.82      0.81      0.81     25000\n",
      "weighted avg       0.82      0.81      0.81     25000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.84852\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.86      0.85     12500\n",
      "           1       0.86      0.84      0.85     12500\n",
      "\n",
      "    accuracy                           0.85     25000\n",
      "   macro avg       0.85      0.85      0.85     25000\n",
      "weighted avg       0.85      0.85      0.85     25000\n",
      "\n",
      "Accuracy:  0.86972\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87     12500\n",
      "           1       0.87      0.87      0.87     12500\n",
      "\n",
      "    accuracy                           0.87     25000\n",
      "   macro avg       0.87      0.87      0.87     25000\n",
      "weighted avg       0.87      0.87      0.87     25000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# note that the third and forth models are both failed to converge\n",
    "clfs = [KNeighborsClassifier(),RandomForestClassifier(),MultinomialNB(),LinearSVC(),LogisticRegression()]\n",
    "\n",
    "for clf in clfs:\n",
    "    clf.fit(X_train_bow, y_train_bow)\n",
    "    # infer the label of each instance in the test set\n",
    "    predictions = clf.predict(X_test_bow)\n",
    "    print('Accuracy: ', accuracy_score(y_test_bow, predictions))\n",
    "    print(classification_report(y_test_bow, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad36cfd5",
   "metadata": {},
   "source": [
    "# Sum of word scores model (General model 2 and genre specific models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "9dc1409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training using sum of rating for each review\n",
    "\n",
    "#f = open('C:/Users/Administrator/Desktop/MBS/Module3/Text and Web Analytics/assignment/our_dataset/aclImdb/train/labeledBow.feat','r')\n",
    "\n",
    "# above for general model and below for genre specific models\n",
    "\n",
    "f = open('C:/Users/Administrator/Desktop/MBS/Module3/Text and Web Analytics/assignment/My work/sort_reviews_by_genres/genres/train/bow/Horror.txt','r')\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for line in f:\n",
    "    fields=re.split(r'\\s+',line)\n",
    "    # Positive reviews have 7 or more stars, negative reviews have <4\n",
    "    if (int(fields[0])>=7):\n",
    "        y_train.append(\"1\")\n",
    "    else:\n",
    "        y_train.append(\"0\")\n",
    "\n",
    "    # this_BOW will store all of the term frequencies for this line in a dictionary\n",
    "    # let's start by attempting to process all of the TDM entries\n",
    "    sum_rating = 0\n",
    "    for i in range(1, len(fields)-1):\n",
    "        index, val = re.split(r':',fields[i])\n",
    "        word = idx2word[int(index)]\n",
    "        rating = word_rating[word] * int(val)\n",
    "        sum_rating += rating\n",
    "        \n",
    "    X_train.append(sum_rating)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "1e95f9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      sum_score  word_count  exclamation_count\n",
      "0      6.455086         171                  0\n",
      "1     11.200551         108                  1\n",
      "2     22.651711         340                  0\n",
      "3     53.365204         436                  0\n",
      "4     22.406493         324                  0\n",
      "...         ...         ...                ...\n",
      "4326  -8.390992         476                  8\n",
      "4327 -21.962934         275                  0\n",
      "4328 -56.947705         779                  1\n",
      "4329  -8.604051         562                  3\n",
      "4330 -16.581549         232                  0\n",
      "\n",
      "[4331 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# extra features 1&2\n",
    "# if run general model, this cell should be skipped\n",
    "\n",
    "count_path = r'C:/Users/Administrator/Desktop/MBS/Module3/Text and Web Analytics/assignment/My work/sort_reviews_by_genres/genres/train/word_count/'\n",
    "count1_path = r'C:/Users/Administrator/Desktop/MBS/Module3/Text and Web Analytics/assignment/My work/sort_reviews_by_genres/genres/train/exclamation_count/'\n",
    "#genre_list = ['Drama', 'Comedy', 'Action', 'Horror', 'Romance', 'Crime', 'Thriller', 'Adventure', 'Fantasy', 'Mystery', 'Sci-Fi', \\\n",
    "#              'Family', 'Biography', 'Music', 'Animation', 'Documentary', 'War', 'History', 'Short', 'Sport', 'Musical', 'Western', \\\n",
    "#              'Film-Noir', 'Reality-TV', 'Talk-Show', 'Adult', 'Game-Show', 'News']\n",
    "\n",
    "with open(count_path + 'Horror.txt', 'r') as f:\n",
    "    temp = []\n",
    "    for count in f:\n",
    "        count = count.strip()\n",
    "        temp.append(int(count))\n",
    "\n",
    "with open(count1_path + 'Horror.txt', 'r') as f:\n",
    "    temp1 = []\n",
    "    for count1 in f:\n",
    "        count1 = count1.strip()\n",
    "        temp1.append(int(count1))\n",
    "\n",
    "X_train = pd.DataFrame({'sum_score':X_train, 'word_count':temp, 'exclamation_count':temp1})\n",
    "\n",
    "print(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "ad009d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "\n",
    "#f = open('C:/Users/Administrator/Desktop/MBS/Module3/Text and Web Analytics/assignment/our_dataset/aclImdb/test/labeledBow.feat','r')\n",
    "\n",
    "# above for general model and below for genre specific models\n",
    "\n",
    "f = open('C:/Users/Administrator/Desktop/MBS/Module3/Text and Web Analytics/assignment/My work/sort_reviews_by_genres/genres/test/bow/Horror.txt','r')\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for line in f:\n",
    "    fields=re.split(r'\\s+',line)\n",
    "    # Positive reviews have 7 or more stars, negative reviews have <4\n",
    "    if (int(fields[0])>=7):\n",
    "            y_test.append(\"1\")\n",
    "    else:\n",
    "            y_test.append(\"0\")\n",
    "\n",
    "    # this_BOW will store all of the term frequencies for this line in a dictionary\n",
    "    # let's start by attempting to process all of the TDM entries\n",
    "    sum_rating = 0\n",
    "    for i in range(1, len(fields)-1):\n",
    "        index, val = re.split(r':',fields[i])\n",
    "        word = idx2word[int(index)]\n",
    "        rating = word_rating[word] * int(val)\n",
    "        sum_rating += rating\n",
    "        \n",
    "    X_test.append(sum_rating)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "7f6ba127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      sum_score  word_count  exclamation_count\n",
      "0     -1.020373         183                  0\n",
      "1     49.220538         674                  1\n",
      "2     12.794878         189                  0\n",
      "3      6.889241         134                  0\n",
      "4      1.052985         301                  0\n",
      "...         ...         ...                ...\n",
      "4492   2.850117         317                  0\n",
      "4493  -0.283095         205                  0\n",
      "4494   6.139030         235                  1\n",
      "4495  -3.269744         209                  0\n",
      "4496  -4.291644         326                  1\n",
      "\n",
      "[4497 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# extra features 1&2\n",
    "# if run general model, this cell should be skipped\n",
    "\n",
    "count_path = r'C:/Users/Administrator/Desktop/MBS/Module3/Text and Web Analytics/assignment/My work/sort_reviews_by_genres/genres/test/word_count/'\n",
    "count1_path = r'C:/Users/Administrator/Desktop/MBS/Module3/Text and Web Analytics/assignment/My work/sort_reviews_by_genres/genres/test/exclamation_count/'\n",
    "#genre_list = ['Drama', 'Comedy', 'Action', 'Horror', 'Romance', 'Crime', 'Thriller', 'Adventure', 'Fantasy', 'Mystery', 'Sci-Fi', \\\n",
    "#              'Family', 'Biography', 'Music', 'Animation', 'Documentary', 'War', 'History', 'Short', 'Sport', 'Musical', 'Western', \\\n",
    "#              'Film-Noir', 'Reality-TV', 'Talk-Show', 'Adult', 'Game-Show', 'News']\n",
    "\n",
    "with open(count_path + 'Horror.txt', 'r') as f:\n",
    "    temp = []\n",
    "    for count in f:\n",
    "        count = count.strip()\n",
    "        temp.append(int(count))\n",
    "\n",
    "with open(count1_path + 'Horror.txt', 'r') as f:\n",
    "    temp1 = []\n",
    "    for count1 in f:\n",
    "        count1 = count1.strip()\n",
    "        temp1.append(int(count1))\n",
    "\n",
    "X_test = pd.DataFrame({'sum_score':X_test, 'word_count':temp, 'exclamation_count':temp1})\n",
    "\n",
    "print(X_test)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "04aab14e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#X_train = np.array(X_train).reshape(-1, 1)\n",
    "#X_test = np.array(X_test).reshape(-1, 1)\n",
    "\n",
    "# above for general model and below for genre specific models\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "7968f7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8003113186568823\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.96      0.88      3500\n",
      "           1       0.63      0.24      0.34       997\n",
      "\n",
      "    accuracy                           0.80      4497\n",
      "   macro avg       0.72      0.60      0.61      4497\n",
      "weighted avg       0.77      0.80      0.76      4497\n",
      "\n",
      "Feature: 0, Score: 0.08467\n",
      "Feature: 1, Score: -0.00044\n",
      "Feature: 2, Score: -0.01519\n"
     ]
    }
   ],
   "source": [
    "# note that the third and forth models are both failed to converge\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "# infer the label of each instance in the test set\n",
    "predictions = clf.predict(X_test)\n",
    "print('Accuracy: ', accuracy_score(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "importance = clf.coef_[0]\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
